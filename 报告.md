# 摘 要

本研究面向在线判题平台（Online Judge, OJ）学习场景，围绕“个性化题目推荐”问题构建了一套从数据整理、特征建模到推荐评估的完整流程。数据由两部分组成：其一为公开 OJ 题库离线整理得到的题目信息（含题目难度与算法类型标签）；其二为基于“用户能力 × 题目难度 × 多次尝试学习效应”的规则模拟生成的提交日志（包含提交序号、用户、题目、语言与判题结果），并在此基础上派生用户画像。实验数据规模为 1000 名用户、10491 道题目与 30 万条提交记录。方法上，基于用户历史提交行为构建可解释特征，涵盖用户能力与坚持度（level、perseverance）、交互强度（attempt_no）、题目难度（difficulty）、以及“语言（单选）/标签（可多选）”的 0/1 特征向量表示，并采用按提交顺序的时间切分（前 80% 训练、后 20% 测试）进行通过概率预测。对比 Logistic Regression、Linear SVM 与 Decision Tree 后，Logistic Regression 在测试集表现最佳（Accuracy=0.683，F1=0.675）。推荐阶段以预测通过概率作为排序分数，并结合“成长型推荐”规则（优先推荐通过概率位于适中区间的题目）生成 Top‑K 列表，在 K=10 时取得 Hit@10=0.082。最后结合可视化结果，从用户活跃度分布、难度与通过率关系以及推荐覆盖度等角度进行解释，验证了所构建方法在 OJ 学习推荐场景中的可行性与可扩展性。

关键词：在线判题（Online Judge）；题目推荐系统；学习行为数据挖掘；通过率预测（Pass Prediction）；Hit@K 评估

# 1 课程内容总结

## 1.1 人工智能课程知识体系

本课题对应的 AI 基本流程可概括为：数据 → 特征 → 模型 → 评估 → 部署。

- 监督学习：以“提交是否 AC”为二分类任务，完成训练/验证/测试（本项目用时间切分替代随机切分）。
- 评价指标：分类任务使用 Accuracy/Precision/Recall/F1；推荐任务使用 Hit@K/Precision@K。
- 数据预处理：缺失值处理、类别特征编码、样本划分、以及“数据泄漏”风险控制。

## 1.2 本课题对应的核心知识点

为体现“课程知识点对齐”，本项目将课堂概念落到可运行脚本与可复现产物上：

- 逻辑回归（Logistic Regression）
  - 核心思想：用线性函数输出 logit，经 sigmoid 得到 `P(AC)`，再用对数损失（log loss）拟合概率。
  - 正则化：通过惩罚项控制模型复杂度，缓解过拟合（sklearn 默认带正则化）。
- 线性 SVM、决策树的对比理由
  - Linear SVM：同属线性模型，适合高维稀疏特征（语言/标签 0/1 向量），对比其分类边界与鲁棒性。
  - Decision Tree：可拟合非线性与特征交互，对比其可解释性与过拟合倾向。
- 特征工程
  - 用户画像：level、perseverance 由历史行为统计得到；偏好（语言/标签）由历史分布得到。
  - 类别编码：语言（单选）转为一组 0/1 列；标签（可多选）转为一组 0/1 列。
- 时间切分与离线评估
  - 按提交顺序做 80/20 切分，确保特征只使用“当前提交之前的历史”，避免“看未来”的泄漏。
- 部署（课程延伸点）
  - 训练得到的 Pipeline 可由 `WebApp/server.py` 加载，提供本地可视化与交互式推荐。

# 2 基于在线判题行为建模的题目推荐应用研究

本章是正文主体，目标是做到“能复现、可解释、有指标、有图”。

## 2.1 实际问题描述

### 2.1.1 预测任务（Pass Prediction）

给定用户 `u` 与题目 `p` 以及该次交互的上下文特征 `x(u,p)`，预测本次提交是否 AC：

- 标签 `y ∈ {0,1}`：`y=1` 表示 AC，`y=0` 表示未 AC。
- 输出概率：模型预测 `P(AC | x(u,p))`，用于后续推荐排序。

### 2.1.2 推荐任务（Top‑K Recommendation）

对每个用户生成一个长度为 K 的题目列表，期望在测试时间窗内能“命中”用户最终 AC 的题目：

- 输入：用户历史已做/已 AC 题目集合与候选题目集合。
- 输出：Top‑K 题目列表（按模型分数排序，可叠加规则过滤）。
- 目标：提高 Hit@K（以及 Precision@K）。

### 2.1.3 场景约束与挑战

- 时间顺序：推荐时只能使用历史信息，离线评估必须避免未来信息泄漏。
- 冷启动：新用户/新题目缺少历史，特征可用性有限（本项目以可复现流程为主，后续可扩展）。
- 多样性/覆盖度：避免推荐过度集中在少数“热门题”或“极易题”。

## 2.2 数据来源说明

### 2.2.1 数据构成与字段

项目以 `CleanData/*.csv` 为输入，关键表如下：

- 题库表 `CleanData/problems.csv`：题目标题、难度（1–10）、标签（算法类型）等。
- 提交表 `CleanData/submissions.csv`：`submission_id/user_id/problem_id/attempt_no/language/verdict/ac` 等。
- 用户表 `CleanData/students.csv`：用户主键与占位字段；画像由脚本派生到 `CleanData/students_derived.csv`。
- 字典表：`CleanData/tags.csv`、`CleanData/languages.csv`、`CleanData/verdicts.csv`。

### 2.2.2 数据真实性边界（研究表述）

为保证可复现与避免隐私风险，本项目将“题库”与“提交日志”分开处理：

- 题库数据：来自公开 OJ 题目数据的离线整理/解析（形成 `problems.csv` 的结构化字段）。
- 标签与难度：可通过离线标注或 LLM（如 Qwen）辅助生成并合并；本仓库提供相关工具脚本（涉及网络调用时需自行配置）。
- 提交日志：使用 `Utils/generate_originaldata_sim.py` 基于可解释的行为生成机制模拟得到（受题目难度、用户能力与尝试次数影响），用于跑通“建模→推荐→评估”的完整闭环。

### 2.2.3 数据规模

本实验所用数据规模如下（来自当前仓库数据文件的统计）：

- 用户数：1000
- 题目数：10491
- 提交数：300000

## 2.3 数据预处理与约束校验

本节重点回答“如何确保数据能用”，对应脚本为 `Utils/validate_originaldata.py`。

### 2.3.1 一致性与完整性校验

- 主键唯一性：`problem_id`、`user_id`、`submission_id` 不重复。
- 外键一致性：
  - `submissions.user_id ∈ students.user_id`
  - `submissions.problem_id ∈ problems.problem_id`
  - `submissions.language ∈ languages.name`
  - `submissions.verdict ∈ verdicts.name`
- 值域约束：
  - `difficulty ∈ [1,10]`
  - `ac ∈ {0,1}` 且与 `verdict=="AC"` 一致
  - `attempt_no` 为正整数，且同一 `(user_id, problem_id)` 内递增

### 2.3.2 缺失值与异常处理策略

- 难度缺失：用默认值填补（在特征侧使用 `difficulty_filled`）。
- 稀疏类别：语言/标签使用 0/1 向量编码，缺失视为全 0 或使用默认分布。
- 异常行：校验脚本可将问题写入报告文件，便于定位修复。

### 2.3.3 预处理产物与可复现命令

- 校验命令：
  - `python Utils/validate_originaldata.py`
  - （可选）`python Utils/validate_originaldata.py --report Reports/validate_report.txt`
- 画像派生命令：`python 03_derive_students.py` → `CleanData/students_derived.csv`

## 2.4 特征工程

特征设计强调“可解释 + 可复现 + 无泄漏”。训练样本由 `python 04_build_features.py` 生成至 `FeatureData/train_samples.csv`，每行对应一次提交。

### 2.4.1 用户侧特征（画像）

- `level`：能力（0–1），基于历史做对题目的难度加权表现归一化得到。
- `perseverance`：坚持度（0–1），基于“人均每题尝试次数”统计并归一化。
- `lang_pref`：历史语言偏好分布（由历史提交语言频率统计）。
- `tag_pref`：历史标签偏好分布（由历史做题标签频率统计）。

### 2.4.2 题目侧特征

- `difficulty_filled`：题目难度（缺失已处理）。
- 标签特征：题目可能同时属于多个标签，用“是否包含该标签”的 0/1 列表示（可多选）。

### 2.4.3 交互侧特征（用户×题目）

- `attempt_no`：同一用户对同一题目的第几次尝试。
- `lang_match`：本次语言与用户历史偏好的匹配程度（越常用越高）。
- `tag_match`：本题标签与用户历史偏好的匹配程度（越偏好越高）。

### 2.4.4 类别编码的口语化说明

- 语言是“单选项”（一次提交只会用一种语言），因此把每种语言展开成一列 0/1：用了就记 1，没用记 0。
- 标签是“可多选”（一道题可以同时属于多个算法类型），因此同样展开成多列 0/1，但一行里可以同时有多列为 1。

## 2.5 模型选择与训练策略

### 2.5.1 候选模型

- Logistic Regression：输出概率，便于直接用于推荐排序（`P(AC)`）。
- Linear SVM：对高维稀疏特征常见且强基线。
- Decision Tree：提供非线性对比，观察是否能捕捉交互效应。

### 2.5.2 数据切分（强调避免泄漏）

采用按 `submission_id` 排序的时间切分（80%/20%）：

- 训练集：较早的 80% 提交
- 测试集：较晚的 20% 提交

该切分方式更贴近真实线上“用过去预测未来”的使用方式，也能更清晰暴露“是否使用了未来信息”的问题。

### 2.5.3 训练细节

对应脚本：`python 05_train_eval.py`

- 标准化：对稀疏特征使用 `StandardScaler(with_mean=False)`（适配 one‑hot/多列 0/1 特征）。
- Logistic Regression：`max_iter=300`，`random_state=42`。
- Decision Tree：`max_depth=10`，`random_state=42`。
- 评价指标：Accuracy、Precision、Recall、F1（报告中优先看 F1，因为它同时兼顾 Precision 与 Recall）。

## 2.6 实验结果与分析

### 2.6.1 分类指标结果

分类评估结果来自 `Models/metrics.csv`（时间切分测试集）：

| 模型 | Accuracy | Precision | Recall | F1 |
| --- | ---:| ---:| ---:| ---:|
| Logistic Regression | 0.683 | 0.681 | 0.670 | 0.675 |
| Linear SVM | 0.683 | 0.681 | 0.669 | 0.675 |
| Decision Tree | 0.674 | 0.672 | 0.661 | 0.666 |

结论：Logistic Regression 在 F1 上略优（或与 Linear SVM 接近），且能输出概率，便于直接用于推荐排序，因此选为后续推荐主模型。

### 2.6.2 混淆矩阵与错误类型

混淆矩阵图已输出到 `Reports/fig_cm_*.png`（或 `Reports/fig_confusion_*.png`）。可从两类典型错误解释模型局限：

- “难题误判为可 AC”：可能来自难度特征粒度不够、或用户能力随时间变化未被完全捕捉。
- “简单题误判为不可 AC”：可能来自语言/标签偏好与该题不匹配、或 attempt_no 较小导致学习效应不足。

### 2.6.3 误差来源（结合数据与任务特点）

- 特征稀疏：标签列维度高且长尾明显，少见标签对模型学习不充分。
- 行为序列效应：用户能力与偏好会随时间变化，静态或弱动态特征难以完全刻画。
- 模拟数据偏差：模拟机制简化了真实 OJ 的复杂因素（例如题目质量、隐藏样例、用户策略差异等）。

## 2.7 推荐策略与评估

### 2.7.1 从预测到推荐的闭环

对应脚本：`python 06_recommend_eval.py`

推荐流程：

1. 训练 `P(AC)` 模型（默认使用 Logistic Regression Pipeline）。
2. 对每个用户构造候选集：从“历史未 AC 的题目”中筛选候选题。
3. 为候选题打分：用模型输出 `p_ac = P(AC|u,p)` 作为排序分数。
4. 规则增强（成长型推荐）：优先选择 `p_ac` 位于适中区间（默认 `[0.4, 0.7]`）的题目，避免“太简单/太困难”。
5. 输出 Top‑K：写入 `Reports/recommendations_topk.csv`。

### 2.7.2 推荐评估指标

按时间窗离线评估（与分类同样的时间切分思想）：

- `Hit@K`：Top‑K 中是否至少有 1 道题在测试窗内被该用户最终 AC（命中=1，否则=0，再对用户取平均）。
- `Precision@K`：Top‑K 中命中 AC 的比例。

### 2.7.3 推荐结果（本项目指标）

在“严格无泄漏（strict）”口径下（cutoff_submission_id=240000，growth_band=[0.4,0.7]），推荐评估结果如下（来自 `Reports/compare_reco_metrics.csv`）：

| K | Hit@K | Precision@K |
| ---:| ---:| ---:|
| 1 | 0.004 | 0.004 |
| 3 | 0.014 | 0.0047 |
| 5 | 0.035 | 0.0074 |
| 10 | 0.082 | 0.0100 |

说明：仓库中还提供了多种对比策略（如热门题、随机等），以及 strict vs leaky 的对比报告 `Reports/compare_strict_vs_leaky_report.md`，用于展示“时间泄漏会虚高指标”的现象与诊断方法。

### 2.7.4 覆盖度与集中度分析

为避免“只推荐少数题/热门题”，项目输出覆盖度相关图表与结果文件：

- `Reports/fig_reco_coverage.png`：推荐集中度/覆盖度可视化。
- `Reports/recommendations_topk.csv`：可统计不同题目被推荐次数分布，观察长尾覆盖情况。

## 2.8 可视化与可解释性

本项目将可视化按三类主题组织，便于写作与答辩展示：

- 数据合理性（行为与分布）
  - `Reports/fig_user_activity.png`：用户活跃度分布
  - `Reports/fig_difficulty_vs_ac.png`：难度与通过率关系
  - `Reports/fig_attemptno_vs_ac.png`：尝试次数与通过率关系
  - `Reports/fig_lang_dist.png`、`Reports/fig_lang_acrate.png`：语言分布与通过率
  - `Reports/fig_tag_dist.png`、`Reports/fig_tag_acrate.png`：标签分布与通过率
- 模型表现
  - `Reports/fig_model_f1_compare.png`：模型 F1 对比
  - `Reports/fig_cm_logreg.png`（及其他模型）：混淆矩阵
- 推荐表现
  - `Reports/fig_hitk_curve.png`：Hit@K 随 K 变化曲线
  - `Reports/fig_reco_coverage.png`：推荐覆盖度/集中度
  - `Reports/fig_reco_difficulty_hist.png`：单用户推荐难度分布示例

# 3 总结与展望

## 3.1 工作总结

本项目完成了一个可复现的 OJ 推荐闭环：

1) 数据整理与校验（字段/外键/值域/时间顺序） →  
2) 用户画像派生（level、perseverance、偏好分布） →  
3) 训练样本构造（提交级别特征与标签） →  
4) 模型训练评估（LogReg/SVM/Tree，对比指标与混淆矩阵） →  
5) Top‑K 推荐生成（以 `P(AC)` 排序 + 成长型规则） →  
6) 推荐评估与可视化（Hit@K、Precision@K、覆盖度等）。

## 3.2 不足与改进方向

- 模拟数据与真实分布差异：真实 OJ 中语言、标签与通过率分布可能更不均衡，且用户策略更复杂；后续应引入真实公开数据或更贴近真实的生成机制。
- 模型能力限制：线性模型对序列行为（近期状态、学习曲线、遗忘效应）表达不足；可尝试 GBDT/XGBoost 或引入序列特征/序列模型。
- 更严格的离线评估：除全局时间切分外，可补充“按用户留出（leave‑one‑out）”“滑动时间窗评估”等，模拟线上推荐迭代。
- 冷启动与多样性：引入题目内容特征（题面文本、知识点结构图）与多样性约束（MMR、coverage regularization）以改善新题/新用户与列表多样性。

# 参考文献

1. C. M. Bishop. *Pattern Recognition and Machine Learning*. Springer, 2006.  
2. T. Hastie, R. Tibshirani, J. Friedman. *The Elements of Statistical Learning*. Springer, 2009.  
3. C. Cortes, V. Vapnik. “Support-Vector Networks”. *Machine Learning*, 1995.  
4. J. R. Quinlan. *C4.5: Programs for Machine Learning*. Morgan Kaufmann, 1993.  
5. J. L. Herlocker, J. A. Konstan, L. G. Terveen, J. T. Riedl. “Evaluating Collaborative Filtering Recommender Systems”. *ACM TOIS*, 2004.  
6. F. Pedregosa et al. “Scikit-learn: Machine Learning in Python”. *JMLR*, 2011.  

# 附 录

## A. 关键字段表（简表）

### A.1 `CleanData/problems.csv`

- `problem_id`：题目 ID（主键）
- `title`：题目标题
- `difficulty`：难度（1–10）
- `tags`：标签（JSON 数组字符串，或由工具归一化）

### A.2 `CleanData/submissions.csv`

- `submission_id`：提交 ID（主键，亦作为时间顺序）
- `user_id`：用户 ID（外键）
- `problem_id`：题目 ID（外键）
- `attempt_no`：同一用户对同一题的第几次尝试（递增）
- `language`：语言（来自 `languages.csv`）
- `verdict`：判题结果（来自 `verdicts.csv`）
- `ac`：是否 AC（0/1）

### A.3 `CleanData/students.csv` / `CleanData/students_derived.csv`

- `user_id`：用户 ID（主键）
- `level` / `perseverance`：画像数值特征（0–1）
- `lang_pref` / `tag_pref`：偏好分布（JSON 字典字符串）

## B. 一键复现实验命令

```bash
python Utils/validate_originaldata.py
python 03_derive_students.py
python 04_build_features.py
python 05_train_eval.py
python 06_recommend_eval.py
python 07_make_eda_plots.py
```

（可选）启动本地 Web 展示图表与自定义推荐：

```bash
python WebApp/server.py --port 8000
```
