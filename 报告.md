# 2 基于监督学习的OJ题目推荐技术的应用研究

## 2.1 实际问题描述

本项目是基于本人的毕业设计实现的一套算法题目推荐系统。系统面向 Online Judge（OJ）学习场景，目标是根据学生的历史做题行为，为其在题库中推荐“下一步更值得做”的题目，并用离线实验给出可复现的指标与图表支撑。

从技术路线看，本项目把推荐问题拆解为“预测 + 排序”的闭环：

1. **通过率预测（监督学习）**：把“某次提交是否 AC”建模为二分类问题，学习一个预测函数输出 `P(AC)`。
2. **Top‑K 推荐**：对每个用户从候选题集中计算 `P(AC)` 并排序，输出 Top‑K 列表；同时支持“成长型推荐”规则（推荐 `P(AC)` 处于适中区间的题，避免只推极易题或极难题）。
3. **离线评估（严格无泄漏）**：按 `submission_id` 做时间切分（前 80% 训练、后 20% 测试），训练窗只用于训练模型/统计画像；测试窗只用于评估命中（ground truth），避免“看未来”导致指标虚高。

对应的两个核心任务定义如下：

### 2.1.1 预测任务（Pass Prediction）

给定用户 `u` 与题目 `p` 以及该次交互的上下文特征 `x(u,p,t)`，预测本次提交是否 AC：

- 标签 `y ∈ {0,1}`：`y=1` 表示 AC，`y=0` 表示未 AC。
- 输出概率：模型预测 `P(AC | x(u,p,t))`，用于后续推荐排序。

### 2.1.2 推荐任务（Top‑K Recommendation）

对每个用户生成一个长度为 K 的题目列表，期望在测试时间窗内能“命中”用户最终 AC 的题目：

- 输入：用户历史已做/已 AC 题目集合与候选题目集合。
- 输出：Top‑K 题目列表（按模型分数排序，可叠加规则过滤）。
- 目标：提高 Hit@K（以及 Precision@K）。

### 2.1.3 场景约束与挑战

- 时间顺序：推荐时只能使用历史信息，离线评估必须避免未来信息泄漏。
- 冷启动：新用户/新题目缺少历史，特征可用性有限（本项目以可复现流程为主，后续可扩展）。
- 多样性/覆盖度：避免推荐过度集中在少数“热门题”或“极易题”。

---

## 2.2 数据来源说明

### 2.2.1 原始数据来源

本项目的题库原始数据来自以下开源数据仓库下载：

https://gitcode.com/open-source-toolkit/0972f/

下载后得到题库压缩包：`OriginalData/题库.zip`。该压缩包为离线题库素材，本项目通过 Python 脚本进行解析与格式化，最终转换为主流水线使用的 `CleanData/*.csv` 标准表。

### 2.2.2 数据处理与生成的工具链（Utils）

从 `OriginalData/题库.zip` 到 `CleanData/` 的主要处理环节如下（均由 `Utils/` 下脚本支持）：

1. **离线解析题库 HTML → CSV**  
`Utils/tk_html_to_csv.py`：把解压后的题面 HTML 解析成结构化题库表（包含 title/description/sample/time_limit/memory_limit 等字段）。

2. **AI 打标签与难度（可选但推荐）**  
`Utils/csv_to_requests.py`：将题面 CSV 生成带 BEGIN/END 标记的 prompts 文件；  
`Utils/batch_label_qwen.py`：调用大模型（需要 API Key，涉及网络）批量生成 `difficulty/tags`；  
`Utils/merge_labels_into_originaldata_problems.py`：将标注结果合并回 `CleanData/problems.csv`；  
`Utils/normalize_problems_tags_json.py`：把 `problems.tags` 规范化为 JSON 数组字符串，并用 `CleanData/tags.csv` 白名单过滤未知标签。

#### 2.2.2.1 AI 标注流程与质量控制（论文说明）

为确保 AI 标注结果“可用、可控、可追溯”，本项目在 prompts 约束、输出校验、失败修复与后处理环节做了明确设计：

1) **强约束的输出格式（prompts 与 system prompt）**  
`Utils/csv_to_requests.py` 生成的 prompts 明确要求：
- 只允许输出一行 JSON
- 只能包含 `difficulty` 与 `tags` 两个键
- `difficulty` 必须为 1–10 的整数
- `tags` 必须为 1–2 个标签，且必须来自可选标签列表（白名单）

`Utils/batch_label_qwen.py` 同时设置 system prompt，进一步要求模型“只能输出那一行 JSON，不能输出任何其它字符”，以减少解释性文字、代码块等噪声输出。

2) **白名单与结构化解析校验**  
`Utils/batch_label_qwen.py` 内置 `ALLOWED_TAGS` 白名单，并在解析阶段对模型输出进行严格校验：
- 解析失败（非 JSON、字段不匹配、difficulty 越界、tags 不在白名单）即判为无效输出
- 只有通过校验的结果才会写回 CSV（或进入后续合并）

3) **失败重试与“自动纠错（repair）”机制**  
当模型输出不符合格式要求时，`Utils/batch_label_qwen.py` 会在原 prompt 基础上追加“系统纠错”段落，重申白名单与输出约束，并进行 `--repair-retries` 次纠错重试（默认 2 次）。这能显著降低“格式错/标签不合法”导致的无效率。

4) **可追溯的断点续跑与错误记录**  
脚本将中间结果以 append-only 的 JSONL 形式落盘（默认 `labels_results.jsonl` / `labels_errors.jsonl`），便于：
- 中断后继续（断点续跑）
- 回溯某题的原始输出与失败原因

5) **后处理合并与格式规范化**  
`Utils/merge_labels_into_originaldata_problems.py` 支持按顺序或按 `problem_id` 合并标注结果，并可输出校验报告；  
`Utils/normalize_problems_tags_json.py` 会把 `problems.tags` 统一为 JSON 数组字符串格式，并按 `CleanData/tags.csv` 白名单过滤未知标签，保证下游特征构造与 one‑hot/multi‑hot 展开维度稳定。

补充说明：AI 标注步骤会产生网络请求，需要配置 `DASHSCOPE_API_KEY`（或通过 `--api-key` 传入），并可通过 `--rpm/--tpm` 做限流以避免触发服务端限速；在无网络环境下可跳过 AI 标注，保留已有标签或使用工具脚本补全。

3. **行为日志与字典表生成（为训练提供 submissions）**  
本项目的训练与推荐依赖 `CleanData/submissions.csv`。在仅有题库而缺少真实提交日志的情况下，本仓库提供两种模拟方案：  
- **多因素相关模拟（主方案）**：`Utils/fill_exec_mem.py generate`，显式模拟能力/难度/偏好匹配/多次尝试学习效应，并生成 `exec_time_ms/mem_kb`；  
- **最小可运行模拟（备选）**：`Utils/generate_originaldata_sim.py`，快速生成 students/languages/verdicts/submissions 以跑通全链路。

4. **一致性校验（门禁）**  
`Utils/validate_originaldata.py`：检查外键/值域/时间顺序等约束，保证 `CleanData/` 可以进入建模阶段。

### 2.2.3 CleanData 标准表与规模

项目以 `CleanData/*.csv` 为输入，关键表如下：

- `CleanData/problems.csv`：题目元信息（含 difficulty/tags 等）。
- `CleanData/submissions.csv`：提交日志（含 `submission_id/user_id/problem_id/attempt_no/language/verdict/ac` 等）。
- `CleanData/students.csv`：用户主键与占位字段；画像由脚本派生到 `CleanData/students_derived.csv`。
- `CleanData/tags.csv`、`CleanData/languages.csv`、`CleanData/verdicts.csv`：标签/语言/判题结果字典表。

当前实验数据规模（以仓库现有 CleanData 统计为准）：

- 用户数：1000
- 题目数：10491
- 提交数：300000

![图2-1 用户活跃度分布（提交次数长尾）](Reports/fig/fig_user_activity.png)

*图2-1 用户活跃度分布：可以观察到提交次数存在明显长尾，这也是推荐系统常见的现象（少数高活跃用户贡献大量交互）。*

---

## 2.3 问题求解

本节对应“数据预处理（清洗、标准化、特征构造）→ 模型选择与训练 → 评估与选型 → 推荐生成与评估 → 可解释性图表”的完整闭环。

### 2.3.1 数据预处理与约束校验

对应脚本：`Utils/validate_originaldata.py`。

核心校验点：

- 外键一致性：
  - `submissions.user_id ∈ students.user_id`
  - `submissions.problem_id ∈ problems.problem_id`
  - `submissions.language ∈ languages.name`
  - `submissions.verdict ∈ verdicts.name`
- 值域约束：
  - `difficulty ∈ [1,10]`（允许缺失；建模侧会补全为 `difficulty_filled`）
  - `ac ∈ {0,1}` 且与 `verdict=="AC"` 等价
  - `attempt_no` 为正整数，且同一 `(user_id, problem_id)` 内严格递增

可复现命令：

- `python Utils/validate_originaldata.py`
- （可选）`python Utils/validate_originaldata.py --report Reports/validate/validate_report.txt`

### 2.3.2 特征构造（无泄漏）

训练样本由 `python 02_build_features.py` 生成至 `FeatureData/train_samples.csv`，每行对应一次提交（submission）。

特征设计强调：
- **可解释**：难度、尝试次数、语言/标签匹配等都能对应学习行为直觉；
- **可复现**：语言/标签词表由 `CleanData/languages.csv` 与 `CleanData/tags.csv` 固定；
- **严格无泄漏**：对每条 submission，只使用该 submission 之前的历史统计构造动态特征。

特征类别（示例）：

- 用户侧动态特征：`level`、`perseverance`
- 题目侧特征：`difficulty_filled`
- 交互侧特征：`attempt_no`、`lang_match`、`tag_match`
- 稀疏编码：语言 one‑hot（`lang_*`）、标签 multi‑hot（`tag_*`）

![图2-2 难度与通过率（AC率）关系](Reports/fig/fig_difficulty_vs_ac.png)

*图2-2 难度 vs 通过率：整体呈负相关，作为“难度字段有效性”的 sanity-check。*

![图2-3 尝试次数与通过率关系（attempt_no 截断到 10）](Reports/fig/fig_attemptno_vs_ac.png)

*图2-3 尝试次数 vs 通过率：可观察到学习效应（多次尝试后通过概率上升）的趋势。*

### 2.3.3 模型选择、训练与评估

对应脚本：`python 03_train_eval.py`。

#### 2.3.3.1 训练/测试切分（时间切分）

采用按 `submission_id` 的时间切分（80%/20%）：
- 训练集：较早的 80% 提交
- 测试集：较晚的 20% 提交

该切分方式更贴近线上“用过去预测未来”的使用方式，也能暴露“是否使用了未来信息”的问题。

#### 2.3.3.2 候选模型与选型依据

候选模型：
- Logistic Regression（可输出概率）
- Linear SVM（线性强基线）
- Decision Tree（非线性对比）

评价指标：
- 分类指标：Accuracy / Precision / Recall / F1（优先看 F1）
- 图表：混淆矩阵（辅助解释错误类型）

离线评估结果来自 `Models/metrics.csv`：

| 模型 | Accuracy | Precision | Recall | F1 |
| --- | ---:| ---:| ---:| ---:|
| Logistic Regression | 0.683 | 0.681 | 0.670 | 0.675 |
| Linear SVM | 0.683 | 0.681 | 0.669 | 0.675 |
| Decision Tree | 0.674 | 0.672 | 0.661 | 0.666 |

结论：Logistic Regression 在 F1 上略优（或与 Linear SVM 接近），且能输出 `P(AC)`，便于直接用于推荐排序，因此选为后续推荐主模型，并保存为 `Models/pipeline_logreg.joblib` 供 Web 推理。

![图2-4 各模型 F1 对比（时间切分测试集）](Reports/fig/fig_model_f1_compare.png)

*图2-4 模型 F1 对比：用于直观比较不同模型的综合分类性能。*

![图2-5 Logistic Regression 混淆矩阵](Reports/fig/fig_cm_logreg.png)

*图2-5 混淆矩阵：直观看到误判结构，辅助分析特征不足与决策阈值敏感性。*

### 2.3.4 Top‑K 推荐生成与离线评估

对应脚本：`python 04_recommend_eval.py`。

推荐流程：
1. 训练 `P(AC)` 模型（默认使用 Logistic Regression Pipeline）。
2. 对每个用户构造候选集：从“历史未 AC 的题目”中筛选候选题。
3. 为候选题打分：用模型输出 `p_ac = P(AC|u,p)` 作为排序分数。
4. 规则增强（成长型推荐）：优先选择 `p_ac` 位于适中区间（默认 `[0.4, 0.7]`）的题目，避免“太简单/太困难”。
5. 输出 Top‑K：写入 `Reports/reco/recommendations_topk.csv`。

离线评估指标（测试窗 ground truth）：
- `Hit@K`：Top‑K 中是否至少有 1 道题在测试窗内被该用户最终 AC。
- `Precision@K`：Top‑K 中命中 AC 的比例。

在 strict 口径下（cutoff_submission_id=240000，growth_band=[0.4,0.7]），推荐评估结果如下（来自 `Reports/compare/compare_reco_metrics.csv`）：

| K | Hit@K | Precision@K |
| ---:| ---:| ---:|
| 1 | 0.004 | 0.004 |
| 3 | 0.014 | 0.0047 |
| 5 | 0.035 | 0.0074 |
| 10 | 0.082 | 0.0100 |

![图2-6 Hit@K 随 K 变化曲线](Reports/fig/fig_hitk_curve.png)

*图2-6 Hit@K 曲线：用于观察不同 K 下的命中率增长趋势，并据此选择合适的推荐列表长度。*

### 2.3.5 覆盖度/集中度与可解释性分析

为避免“只推荐少数题/热门题”，项目输出覆盖度相关图表与结果文件：
- `Reports/fig/fig_reco_coverage.png`：推荐集中度/覆盖度可视化。
- `Reports/reco/recommendations_topk.csv`：可统计不同题目被推荐次数分布，观察长尾覆盖情况。

![图2-7 推荐覆盖度/集中度可视化](Reports/fig/fig_reco_coverage.png)

*图2-7 覆盖度/集中度：若推荐过度集中在少数题，说明存在同质化风险，需要加入多样性约束或业务规则。*

![图2-8 单用户推荐难度分布示例](Reports/fig/fig_reco_difficulty_hist.png)

*图2-8 单用户案例：用于检查推荐是否过度偏简单或过难，并与该用户历史做题难度对照。*

---

# 3 总结

本项目完成了一个从 `OriginalData/题库.zip` 出发、通过 Python 工具脚本完成格式化/清洗/标注/数据生成，并最终在 `CleanData/` 上进行训练与评估的推荐系统闭环。通过本次实现，我的主要收获与体会包括：

- 能把监督学习的流程落到工程可复现产物：数据→特征→模型→评估→推荐→可视化；
- 深刻理解“时间切分与信息泄漏”对离线指标的影响，并用 strict 口径保证评估可信；
- 学会用可解释特征（难度、尝试次数、偏好匹配）与图表联动分析模型与推荐表现。

在课设/实现过程中遇到的主要困难与解决方案：

1) **原始题库格式不统一（HTML/字段缺失/编码问题）**：通过 `Utils/tk_html_to_csv.py` 做离线解析与统一字段输出。  
2) **标签与难度缺失/格式不一致**：提供“AI 打标签→合并→白名单过滤→JSON 规范化”的工具链（`Utils/csv_to_requests.py`、`Utils/batch_label_qwen.py`、`Utils/merge_labels_into_originaldata_problems.py`、`Utils/normalize_problems_tags_json.py`）。  
3) **缺少真实提交日志**：通过 `Utils/fill_exec_mem.py generate` 构建多因素相关的模拟 submissions（并生成 exec/mem），保证训练与推荐闭环可复现。  
4) **推荐偏简单/同质化**：加入成长带规则，并用覆盖度/单用户难度分布图进行诊断与解释。

后续可改进方向：
- 引入更贴近真实的公开日志数据或更精细的行为生成机制（遗忘效应、题目质量差异等）。
- 尝试更强的非线性模型（GBDT/XGBoost）或加入序列特征，以刻画能力随时间变化。
- 在推荐列表中加入多样性约束与冷启动策略，提升覆盖度与长期学习收益。
